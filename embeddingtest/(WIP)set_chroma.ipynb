{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Chromadb locally(to compare model by score)\n",
    "---\n",
    "### RUN ONLY ONCE IF YOU DON'T HAVE DIRECTORY embeddingtest/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_model_names() -> list:\n",
    "    try:\n",
    "        with open(file=\"model/model_list.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "            model_list = [line.strip() for line in file]\n",
    "    except:\n",
    "        print(\"\"\"file not exsist. check directory or file.\"\"\")\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "### from_document method 이용해서 저장(document, embedding, persist_directory, collection_name)\n",
    "#Chroma object 생성.\n",
    "chroma = Chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize class takes 0.0 seconds.\n",
      "initialize class takes 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Pre-load document\n",
    "from document.mdLoader import TeamALoader, TeamBLoader\n",
    "\n",
    "#document parsing when get max_seq_length -> use in chroma.from_documents()\n",
    "#일단 split 없이 진행하고, chroma db에 넣을 때 sequence에 맞춰서 split 해 줄 것(편의성을 위해 미리 불러온다.)\n",
    "a_loader = TeamALoader(path_db=\"data/teamA\", path_metadata=\"document/meta_team_a.json\", path_url_table=\"document/url_table_team_a.csv\", text_splitter=None)\n",
    "b_loader = TeamBLoader(path_db=\"data/teamB\", path_metadata=\"document/meta_team_b.json\", path_url_table=\"document/url_table_team_b.csv\", text_splitter=None)\n",
    "\n",
    "### splitter 넣어서 하자........... seq 구하고 -> 이건 model loading 필요하니까 결국... loading하고\n",
    "## None으로 시작한 다음에 Splitter 넣어서 돌아가게 하고 다음에 수정해서 하나의 .py로\n",
    "\n",
    "# a_raw_docs = a_loader.load(is_split=False, is_regex=False)\n",
    "# b_raw_docs = b_loader.load(is_split=False, is_regex=True)\n",
    "\n",
    "# print(len(a_raw_docs), len(b_raw_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Loading\n",
    "from embedding import EmbeddingLoader\n",
    "\n",
    "ste_embedding = EmbeddingLoader.SentenceTransformerEmbedding\n",
    "openai_embedding = EmbeddingLoader.OpenAIEmbedding\n",
    "\n",
    "# UseCase\n",
    "# ste_embedding()\n",
    "# openai_embedding()\n",
    "\n",
    "# get model names\n",
    "model_list = get_hf_model_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Embedding Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter, TokenTextSplitter #STE, OpenAIEmbedding(@text-ada-002)\n",
    "import os\n",
    "import json\n",
    "\n",
    "def set_text_splitter(ste_model, max_seq_length)->SentenceTransformersTokenTextSplitter:\n",
    "    splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=10, model_name=ste_model, tokens_per_chunk=max_seq_length, seperator=\"\\n\\n\")\n",
    "    return splitter\n",
    "\n",
    "def get_max_seq_length(model_path)->int:\n",
    "    sentence_bert_config = \"sentence_bert_config.json\"\n",
    "    config_path = os.path.join(model_path, sentence_bert_config)\n",
    "\n",
    "    with open(config_path) as file :\n",
    "        bert_config = json.load(file)\n",
    "        \n",
    "    return bert_config[\"max_seq_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding model in path <model/sentence-transformers/paraphrase-multilingual-mpnet-base-v2> has been loaded successfully.\n",
      "Function call load took 13.601368s to run.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [01:12<00:00,  1.76s/it]\n",
      "100%|██████████| 36/36 [00:04<00:00,  7.31it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.74it/s]\n",
      "100%|██████████| 57/57 [00:05<00:00, 10.56it/s]\n",
      "100%|██████████| 27/27 [00:03<00:00,  8.38it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.94it/s]\n",
      "100%|██████████| 83/83 [00:08<00:00, 10.22it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.13it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.66it/s]\n",
      "100%|██████████| 86/86 [00:10<00:00,  8.30it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'국민임대주택공급국민임대주택공급의대상은입주자모집공고일현재무주택세대구성원으로서소득및자산보유기준을충족하는사람입니다소득기준은도시근로자월평균소득701인가구902인가구80이하입니다또한자산보유기준은총자산이3억6100만원이하자동차3683만원이하입니다국민임대주택공급의내용은전용면적60m2이하주택을시중전세시세의60이상80이하수준으로저렴하게임대'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m a_loader\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m text_splitter\n\u001b[0;32m     20\u001b[0m b_loader\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m text_splitter\n\u001b[1;32m---> 22\u001b[0m a_raw_docs \u001b[38;5;241m=\u001b[39m \u001b[43ma_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m b_raw_docs \u001b[38;5;241m=\u001b[39m b_loader\u001b[38;5;241m.\u001b[39mload(is_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal document length : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(a_raw_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b_raw_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\git\\additional-work\\embeddingtest\\document\\mdLoader.py:64\u001b[0m, in \u001b[0;36mBaseDBLoader.load\u001b[1;34m(self, is_split, is_regex, show_progress, use_multithreading)\u001b[0m\n\u001b[0;32m     62\u001b[0m         doc_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39msplit_documents(doc_list)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mextend(doc_list)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_document_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#timecheck\u001b[39;00m\n\u001b[0;32m     67\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32md:\\git\\additional-work\\embeddingtest\\document\\mdLoader.py:130\u001b[0m, in \u001b[0;36mBaseDBLoader._process_document_metadata\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    128\u001b[0m title \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    129\u001b[0m title_parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_replace_text(title)\n\u001b[1;32m--> 130\u001b[0m document\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata_json\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle_parsed\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m#### url\u001b[39;00m\n\u001b[0;32m    133\u001b[0m result \u001b[38;5;241m=\u001b[39m url_table\u001b[38;5;241m.\u001b[39mloc[url_table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m meta_source_parsed_file_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: '국민임대주택공급국민임대주택공급의대상은입주자모집공고일현재무주택세대구성원으로서소득및자산보유기준을충족하는사람입니다소득기준은도시근로자월평균소득701인가구902인가구80이하입니다또한자산보유기준은총자산이3억6100만원이하자동차3683만원이하입니다국민임대주택공급의내용은전용면적60m2이하주택을시중전세시세의60이상80이하수준으로저렴하게임대'"
     ]
    }
   ],
   "source": [
    "## HuggingFaceEmbedding Setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "directory = \"model/\"\n",
    "sentence_bert_config = \"sentence_bert_config.json\"\n",
    "\n",
    "for model in model_list:\n",
    "    # load model from locally saved HuggingFace model\n",
    "    model_path = os.path.join(directory, model)\n",
    "    \n",
    "    sentenceloader = ste_embedding(model_name=model_path, multi_process=True, encode_kwargs={'normalize_embeddings':True})\n",
    "    embedding_model = sentenceloader.load()\n",
    "\n",
    "    max_seq_length = get_max_seq_length(model_path=model_path)\n",
    "    text_splitter = set_text_splitter(model_path, max_seq_length=max_seq_length)\n",
    "\n",
    "    a_loader.text_splitter = text_splitter\n",
    "    b_loader.text_splitter = text_splitter\n",
    "\n",
    "    a_raw_docs = a_loader.load(is_split=True, is_regex=False)\n",
    "    b_raw_docs = b_loader.load(is_split=True, is_regex=True)\n",
    "\n",
    "    print(f\"total document length : {len(a_raw_docs)}, {len(b_raw_docs)}\")\n",
    "\n",
    "    # get max sequence length from embedding model\n",
    "    config_path = os.path.join(model_path, sentence_bert_config)\n",
    "    with open(config_path) as file :\n",
    "        bert_config = json.load(file)\n",
    "        max_seq_length = bert_config[\"max_seq_length\"]\n",
    "\n",
    "    # set model name(cause collection name length limit)\n",
    "    model_name = model.split(\"/\")[-1]\n",
    "\n",
    "    # save document with chunk - embedding calculate and save it to persist directory\n",
    "    chroma.from_documents(documents=a_raw_docs, embedding=embedding_model, collection_name=model_name+\"-a\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")\n",
    "    chroma.from_documents(documents=b_raw_docs, embedding=embedding_model, collection_name=model_name+\"-b\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Embedding Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 여기도 max sequence 찾아서 해야 할 듯......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embedding has been activated.\n",
      "Function call load took 0.0s to run.\n",
      "\n",
      "<langchain.text_splitter.TokenTextSplitter object at 0x0000017CD7ED8880>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "loader = openai_embedding()\n",
    "emb_openai = loader.load()\n",
    "\n",
    "openai_text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=emb_openai.model\n",
    ")\n",
    "\n",
    "a_loader.text_splitter = openai_text_splitter\n",
    "b_loader.text_splitter = openai_text_splitter\n",
    "\n",
    "a_raw_docs = a_loader.load(is_split=True, is_regex=False)\n",
    "b_raw_docs = b_loader.load(is_split=True, is_regex=True)\n",
    "\n",
    "chroma.from_documents(documents=a_raw_docs, embedding=emb_openai, collection_name=emb_openai.model+\"-a\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")\n",
    "chroma.from_documents(documents=b_raw_docs, embedding=emb_openai, collection_name=emb_openai.model+\"-b\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
