{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Chromadb locally(to compare model by score)\n",
    "---\n",
    "### RUN ONLY ONCE IF YOU DON'T HAVE DIRECTORY embeddingtest/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_model_names() -> list:\n",
    "    try:\n",
    "        with open(file=\"model/model_list.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "            model_list = [line.strip() for line in file]\n",
    "    except:\n",
    "        print(\"\"\"file not exsist. check directory or file.\"\"\")\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "### from_document method 이용해서 저장(document, embedding, persist_directory, collection_name)\n",
    "#Chroma object 생성.\n",
    "chroma = Chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize class takes 0.0 seconds.\n",
      "initialize class takes 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Pre-load document\n",
    "from document.mdLoader import TeamALoader, TeamBLoader\n",
    "\n",
    "#document parsing when get max_seq_length -> use in chroma.from_documents()\n",
    "#일단 split 없이 진행하고, chroma db에 넣을 때 sequence에 맞춰서 split 해 줄 것(편의성을 위해 미리 불러온다.)\n",
    "a_loader = TeamALoader(path_db=\"data/teamA\", path_metadata=\"document/meta_team_a.json\", path_url_table=\"document/url_table_team_a.csv\", text_splitter=None)\n",
    "b_loader = TeamBLoader(path_db=\"data/teamB\", path_metadata=\"document/meta_team_b.json\", path_url_table=\"document/url_table_team_b.csv\", text_splitter=None)\n",
    "\n",
    "### splitter 넣어서 하자........... seq 구하고 -> 이건 model loading 필요하니까 결국... loading하고\n",
    "## None으로 시작한 다음에 Splitter 넣어서 돌아가게 하고 다음에 수정해서 하나의 .py로\n",
    "\n",
    "# a_raw_docs = a_loader.load(is_split=False, is_regex=False)\n",
    "# b_raw_docs = b_loader.load(is_split=False, is_regex=True)\n",
    "\n",
    "# print(len(a_raw_docs), len(b_raw_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Loading\n",
    "from embedding import EmbeddingLoader\n",
    "\n",
    "ste_embedding = EmbeddingLoader.SentenceTransformerEmbedding\n",
    "openai_embedding = EmbeddingLoader.OpenAIEmbedding\n",
    "\n",
    "# UseCase\n",
    "# ste_embedding()\n",
    "# openai_embedding()\n",
    "\n",
    "# get model names\n",
    "model_list = get_hf_model_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Embedding Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter, TokenTextSplitter #STE, OpenAIEmbedding(@text-ada-002)\n",
    "import os\n",
    "import json\n",
    "\n",
    "def set_text_splitter(ste_model, max_seq_length)->SentenceTransformersTokenTextSplitter:\n",
    "    splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=10, model_name=ste_model, tokens_per_chunk=max_seq_length, seperator=\"\\n\\n\")\n",
    "    return splitter\n",
    "\n",
    "def get_max_seq_length(model_path)->int:\n",
    "    sentence_bert_config = \"sentence_bert_config.json\"\n",
    "    config_path = os.path.join(model_path, sentence_bert_config)\n",
    "\n",
    "    with open(config_path) as file :\n",
    "        bert_config = json.load(file)\n",
    "        \n",
    "    return bert_config[\"max_seq_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding model in path <model/sentence-transformers/paraphrase-multilingual-mpnet-base-v2> has been loaded successfully.\n",
      "Function call load took 6.481762s to run.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextSplitter.__init__() got an unexpected keyword argument 'seperator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m sentenceloader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     16\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m get_max_seq_length(model_path\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m---> 17\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mset_text_splitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m a_loader\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m text_splitter\n\u001b[0;32m     20\u001b[0m b_loader\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m text_splitter\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mset_text_splitter\u001b[1;34m(ste_model, max_seq_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_text_splitter\u001b[39m(ste_model, max_seq_length)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mSentenceTransformersTokenTextSplitter:\n\u001b[1;32m----> 6\u001b[0m     splitter \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformersTokenTextSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mste_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseperator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m splitter\n",
      "File \u001b[1;32md:\\anaconda\\envs\\langchain\\lib\\site-packages\\langchain\\text_splitter.py:730\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[1;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    724\u001b[0m     chunk_overlap: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    728\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, chunk_overlap\u001b[38;5;241m=\u001b[39mchunk_overlap)\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[1;31mTypeError\u001b[0m: TextSplitter.__init__() got an unexpected keyword argument 'seperator'"
     ]
    }
   ],
   "source": [
    "## HuggingFaceEmbedding Setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "directory = \"model/\"\n",
    "sentence_bert_config = \"sentence_bert_config.json\"\n",
    "\n",
    "for model in model_list:\n",
    "    # load model from locally saved HuggingFace model\n",
    "    model_path = os.path.join(directory, model)\n",
    "    \n",
    "    sentenceloader = ste_embedding(model_name=model_path, multi_process=True, encode_kwargs={'normalize_embeddings':True})\n",
    "    embedding_model = sentenceloader.load()\n",
    "\n",
    "    max_seq_length = get_max_seq_length(model_path=model_path)\n",
    "    text_splitter = set_text_splitter(model_path, max_seq_length=max_seq_length)\n",
    "\n",
    "    a_loader.text_splitter = text_splitter\n",
    "    b_loader.text_splitter = text_splitter\n",
    "\n",
    "    a_raw_docs = a_loader.load(is_split=True, is_regex=False)\n",
    "    b_raw_docs = b_loader.load(is_split=True, is_regex=True)\n",
    "\n",
    "    print(f\"total document length : {len(a_raw_docs)}, {len(b_raw_docs)}\")\n",
    "\n",
    "    # get max sequence length from embedding model\n",
    "    config_path = os.path.join(model_path, sentence_bert_config)\n",
    "    with open(config_path) as file :\n",
    "        bert_config = json.load(file)\n",
    "        max_seq_length = bert_config[\"max_seq_length\"]\n",
    "\n",
    "    # set model name(cause collection name length limit)\n",
    "    model_name = model.split(\"/\")[-1]\n",
    "\n",
    "    # save document with chunk - embedding calculate and save it to persist directory\n",
    "    chroma.from_documents(documents=a_raw_docs, embedding=embedding_model, collection_name=model_name+\"-a\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")\n",
    "    chroma.from_documents(documents=b_raw_docs, embedding=embedding_model, collection_name=model_name+\"-b\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Embedding Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 여기도 max sequence 찾아서 해야 할 듯......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embedding has been activated.\n",
      "Function call load took 0.0s to run.\n",
      "\n",
      "<langchain.text_splitter.TokenTextSplitter object at 0x0000017CD7ED8880>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "loader = openai_embedding()\n",
    "emb_openai = loader.load()\n",
    "\n",
    "openai_text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=emb_openai.model\n",
    ")\n",
    "\n",
    "a_loader.text_splitter = openai_text_splitter\n",
    "b_loader.text_splitter = openai_text_splitter\n",
    "\n",
    "a_raw_docs = a_loader.load(is_split=True, is_regex=False)\n",
    "b_raw_docs = b_loader.load(is_split=True, is_regex=True)\n",
    "\n",
    "chroma.from_documents(documents=a_raw_docs, embedding=emb_openai, collection_name=emb_openai.model+\"-a\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")\n",
    "chroma.from_documents(documents=b_raw_docs, embedding=emb_openai, collection_name=emb_openai.model+\"-b\", collection_metadata={\"hnsw:space\":\"cosine\"}, persist_directory=\"chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
